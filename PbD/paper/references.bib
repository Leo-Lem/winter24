@article{goldwasser2022backdoors,
  author       = {Shafi Goldwasser and Michael P. Kim and Vinod Vaikuntanathan and Or Zamir},
  title        = {Planting Undetectable Backdoors in Machine Learning Models},
  journal      = {Cryptology ePrint Archive, Paper 2022/1000},
  year         = {2022},
  url          = {https://eprint.iacr.org/2022/1000},
  note         = {The paper explores techniques for embedding undetectable backdoors into machine learning models, providing both cryptographic definitions and practical implementations.}
}

@inproceedings{carlini2017backdoor,
  author       = {Nicholas Carlini and David Wagner},
  title        = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  booktitle    = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec)},
  year         = {2017},
  publisher    = {ACM},
  note         = {This work focuses on the challenge of detecting adversarial examples, which are closely related to undetectable backdoors.}
}

@inproceedings{gu2017badnets,
  author       = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title        = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  booktitle    = {Proceedings of the Workshop on Artificial Intelligence and Security (AISec)},
  year         = {2017},
  organization = {ACM},
  note         = {Introduces a framework for understanding how backdoors can be embedded into machine learning models during the training process.}
}

@article{song2017machine,
  author       = {Congzheng Song and Thomas Ristenpart and Vitaly Shmatikov},
  title        = {Machine Learning Models That Remember Too Much},
  journal      = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)},
  year         = {2017},
  note         = {Discusses how machine learning models can unintentionally or maliciously memorize training data, a related aspect of backdoors.}
}

@article{liu2018trojaning,
  author       = {Yingqi Liu and Shiqing Ma and Yousra Aafer and Wen-Chuan Lee and Juan Zhai and Weiyu Wang and Xiangyu Zhang},
  title        = {Trojaning Attack on Neural Networks},
  journal      = {Proceedings of the 25th Annual Network and Distributed System Security Symposium (NDSS)},
  year         = {2018},
  note         = {Examines how neural networks can be maliciously altered to exhibit undesired behaviors when triggered.}
}

@inproceedings{madry2018adversarial,
  author       = {Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
  title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2018},
  note         = {Proposes techniques for adversarial robustness, which are relevant for mitigating some effects of undetectable backdoors.}
}