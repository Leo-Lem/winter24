@article{goldwasser2022backdoors,
  author       = {Shafi Goldwasser and Michael P. Kim and Vinod Vaikuntanathan and Or Zamir},
  title        = {Planting Undetectable Backdoors in Machine Learning Models},
  journal      = {Cryptology ePrint Archive, Paper 2022/1000},
  year         = {2022},
  url          = {https://eprint.iacr.org/2022/1000},
}

@inproceedings{bruna2021continuous,
  author       = {J. Bruna and O. Regev and M. J. Song and Y. Tang},
  title        = {Continuous LWE},
  booktitle    = {STOC '21: 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  editor       = {S. Khuller and V. V. Williams},
  year         = {2021},
  address      = {Virtual Event, Italy},
  month        = {June 21--25},
  publisher    = {ACM},
  pages        = {694--707},
  note         = {Explores continuous variants of the Learning With Errors (LWE) problem.}
}

@inproceedings{berthet2013complexity,
  author       = {Q. Berthet and P. Rigollet},
  title        = {Complexity Theoretic Lower Bounds for Sparse Principal Component Detection},
  booktitle    = {COLT 2013 - The 26th Annual Conference on Learning Theory},
  editor       = {S. Shalev-Shwartz and I. Steinwart},
  series       = {JMLR Workshop and Conference Proceedings},
  volume       = {30},
  year         = {2013},
  address      = {Princeton University, NJ, USA},
  month        = {June 12--14},
  publisher    = {JMLR.org},
  pages        = {1046--1066},
  url          = {http://proceedings.mlr.press/v30/Berthet13.html},
  note         = {Presents complexity theoretic lower bounds for detecting sparse principal components.}
}

@inproceedings{brennan2019optimal,
  author       = {M. Brennan and G. Bresler},
  title        = {Optimal Average-Case Reductions to Sparse PCA: From Weak Assumptions to Strong Hardness},
  booktitle    = {Conference on Learning Theory},
  year         = {2019},
  publisher    = {PMLR},
  pages        = {469--470},
  note         = {Investigates average-case reductions to sparse PCA under varying assumptions.}
}
%Leon
@InProceedings{10.1007/3-540-69053-0_6,
  author="Young, Adam
  and Yung, Moti",
  editor="Fumy, Walter",
  title="Kleptography: Using Cryptography Against Cryptography",
  booktitle="Advances in Cryptology --- EUROCRYPT '97",
  year="1997",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="62--74",
  abstract="The notion of a Secretly Embedded Trapdoor with Universal Protection (SETUP) has been recently introduced. In this paper we extend the study of stealing information securely and subliminally from black-box cryptosystems. The SETUP mechanisms presented here, in contrast with previous ones, leak secret key information without using an explicit subliminal channel. This extends this area of threats, which we call ``kleptography''.",
  isbn="978-3-540-69053-5"
}
@InProceedings{pmlr-v97-bubeck19a,  
  title =  {Adversarial examples from computational constraints},  
  author =       {Bubeck, Sebastien and Lee, Yin Tat and Price, Eric and Razenshteyn, Ilya},  
  booktitle =  {Proceedings of the 36th International Conference on Machine Learning},  
  pages =  {831--840},  
  year =  {2019},  
  editor =  {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},  
  volume =  {97},  
  series =  {Proceedings of Machine Learning Research},  month =  {09--15 Jun},  
  publisher =    {PMLR},  
  pdf =  {http://proceedings.mlr.press/v97/bubeck19a/bubeck19a.pdf},  
  url =  {https://proceedings.mlr.press/v97/bubeck19a.html},  
  abstract =  {Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.}
}
@InProceedings{pmlr-v117-garg20a,  title =  {Adversarially Robust Learning Could Leverage Computational Hardness.},  author =       {Garg, Sanjam and Jha, Somesh and Mahloujifar, Saeed and Mohammad, Mahmoody},  booktitle =  {Proceedings of the 31st International Conference  on Algorithmic Learning Theory},  pages =  {364--385},  year =  {2020},  editor =  {Kontorovich, Aryeh and Neu, Gergely},  volume =  {117},  series =  {Proceedings of Machine Learning Research},  month =  {08 Feb--11 Feb},  publisher =    {PMLR},  pdf =  {http://proceedings.mlr.press/v117/garg20a/garg20a.pdf},  url =  {https://proceedings.mlr.press/v117/garg20a.html},  abstract =  {Over recent years, devising classification algorithms that are robust to adversarial perturbations has emerged as a challenging problem. In particular, deep neural nets (DNNs) seem to be susceptible to small imperceptible changes over test instances. However, the line of work in <em>provable</em> robustness, so far, has been focused on <em>information theoretic</em> robustness, ruling out even the <em>existence</em> of any adversarial examples. In this work, we study whether there is a hope to benefit from <em>algorithmic</em> nature of an attacker that searches for adversarial examples, and ask whether there is <em>any</em>  learning task for which it is possible to design classifiers that are only robust against <em>polynomial-time</em> adversaries. Indeed, numerous cryptographic tasks (e.g. encryption of long messages) can only be secure against computationally bounded adversaries, and are indeed <em>impossible</em> for computationally unbounded attackers. Thus, it is natural to ask if the same strategy could help robust learning. We show that computational limitation of attackers can indeed be useful in robust learning by demonstrating the possibility of a classifier for some learning task  for which computational and information theoretic adversaries of bounded perturbations have very different power. Namely, while computationally unbounded adversaries can attack successfully and find adversarial examples with small perturbation, polynomial time adversaries are unable to do so   unless they can break standard cryptographic hardness assumptions.  Our results, therefore, indicate that perhaps a similar approach to cryptography (relying on computational hardness) holds promise for achieving computationally robust machine learning.  On the reverse directions, we also show that the existence of such learning task in which computational robustness beats information theoretic robustness requires computational hardness by implying (average-case) hardness of NP.}}

@inproceedings{NEURIPS2022_3538a22c,
 author = {Hong, Sanghyun and Carlini, Nicholas and Kurakin, Alexey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {8068--8080},
 publisher = {Curran Associates, Inc.},
 title = {Handcrafted Backdoors in Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3538a22cd3ceb8f009cc62b9e535c29f-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@InProceedings{pmlr-v139-hayase21a,
  title = 	 {SPECTRE: defending against backdoor attacks using robust statistics},
  author =       {Hayase, Jonathan and Kong, Weihao and Somani, Raghav and Oh, Sewoong},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4129--4139},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hayase21a/hayase21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hayase21a.html},
  abstract = 	 {Modern machine learning increasingly requires training on a large collection of data from multiple sources, not all of which can be trusted. A particularly frightening scenario is when a small fraction of corrupted data changes the behavior of the trained model when triggered by an attacker-specified watermark. Such a compromised model will be deployed unnoticed as the model is accurate otherwise. There has been promising attempts to use the intermediate representations of such a model to separate corrupted examples from clean ones. However, these methods require a significant fraction of the data to be corrupted, in order to have strong enough signal for detection. We propose a novel defense algorithm using robust covariance estimation to amplify the spectral signature of corrupted data. This defense is able to completely remove backdoors whenever the benchmark backdoor attacks are successful, even in regimes where previous methods have no hope for detecting poisoned examples.}
}

@misc{moitra2022spoofinggeneralizationcanttrust,
      title={Spoofing Generalization: When Can't You Trust Proprietary Models?}, 
      author={Ankur Moitra and Elchanan Mossel and Colin Sandon},
      year={2022},
      eprint={2106.08393},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.08393}, 
}

@INPROCEEDINGS{4031390,
  author={Klivans, Adam R. and Sherstov, Alexander A.},
  booktitle={2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)}, 
  title={Cryptographic Hardness for Learning Intersections of Halfspaces}, 
  year={2006},
  volume={},
  number={},
  pages={553-562},
  keywords={Polynomials;Circuits;Arithmetic;Lattices;Neural networks;Quantum mechanics;Computer science;Learning;Public key cryptography;Boolean functions},
  doi={10.1109/FOCS.2006.24}}

@ARTICLE{8685687,
  author={Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={IEEE Access}, 
  title={BadNets: Evaluating Backdooring Attacks on Deep Neural Networks}, 
  year={2019},
  volume={7},
  number={},
  pages={47230-47244},
  keywords={Training;Machine learning;Perturbation methods;Computational modeling;Biological neural networks;Security;Computer security;machine learning;neural networks},
  doi={10.1109/ACCESS.2019.2909068}}

@inproceedings{carlini2017backdoor,
  author       = {Nicholas Carlini and David Wagner},
  title        = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
  booktitle    = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec)},
  year         = {2017},
  publisher    = {ACM},
  note         = {This work focuses on the challenge of detecting adversarial examples, which are closely related to undetectable backdoors.}
}

@inproceedings{gu2017badnets,
  author       = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title        = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  booktitle    = {Proceedings of the Workshop on Artificial Intelligence and Security (AISec)},
  year         = {2017},
  organization = {ACM},
  note         = {Introduces a framework for understanding how backdoors can be embedded into machine learning models during the training process.}
}

@article{song2017machine,
  author       = {Congzheng Song and Thomas Ristenpart and Vitaly Shmatikov},
  title        = {Machine Learning Models That Remember Too Much},
  journal      = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)},
  year         = {2017},
  note         = {Discusses how machine learning models can unintentionally or maliciously memorize training data, a related aspect of backdoors.}
}

@article{liu2018trojaning,
  author       = {Yingqi Liu and Shiqing Ma and Yousra Aafer and Wen-Chuan Lee and Juan Zhai and Weiyu Wang and Xiangyu Zhang},
  title        = {Trojaning Attack on Neural Networks},
  journal      = {Proceedings of the 25th Annual Network and Distributed System Security Symposium (NDSS)},
  year         = {2018},
  note         = {Examines how neural networks can be maliciously altered to exhibit undesired behaviors when triggered.}
}