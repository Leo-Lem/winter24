\documentclass[
	fontsize=12pt,
	headings=small,
	parskip=quarter,
	bibliography=totoc,
	numbers=noenddot,       
	open=any,               
 	final                   
]{scrreprt}
\input{template}

\bibliographystyle{plain}

\title{Planting Undetectable Backdoors in Machine Learning Models summarised}
%\author{Leo Landowski \\ Leon Langhoff \\ Leopold Lemmermann \\ Noah Nehring }

\begin{document}

\begin{titlepage}
	\thispagestyle{empty}
	\mbox{\parbox[t][1.75cm][b]{2.2cm}{\uhhlogo}}
	\begin{center}\Large
		\vfill
		\makeatletter
		{\Large\textsf{\textbf{\@title}}\par}
		\makeatother
		\bigskip
		Privacy by Design \par
		\bigskip
		\makeatletter
		{\@author} \par
		\makeatother
		\bigskip
		\makeatletter
		{\@date}
		\makeatother
		\vfill
		\vfill
	\end{center}
\end{titlepage}

\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\pagenumbering{arabic}
\setcounter{page}{0}


% TODO: reconsider sectioning?
\chapter{Introduction}
The scientific paper "Planting Undetectable Backdoors in Machine Learning Models" by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir \cite{goldwasser2022backdoors} investigates how malicious actors can embed undetectable backdoors in machine learning models to covertly manipulate their classifications. All the information presented in this introduction is based on the findings and arguments presented in the paper. The paper addresses the growing use of machine learning models throughout various domains. Companies can outsource tasks connected to these machine learning models to service providers, thereby democratizing access to machine learning models yet raising serious trust concerns. While using a service provider, a user must trust their claims about accuracy and robustness because, as the paper will present, a service provider can plant an undetectable backdoor into the model. The paper provides a cryptographic framework for understanding and constructing these backdoors and shows how a malicious entity can implement them. It doesn’t matter for the company if there is white or black-box access to the machine learning model; these backdoors are undetectable, and the already mentioned framework ensures the undetectability.
\par The paper introduces definitions for  
\begin{enumerate} \itemsep -5pt
	\item \textbf{Black-box undetectability}: Where the user has only access to the model's output, making it difficult to identify backdoors. 
	\item \textbf{White-box undetectability}: Where the user has full access to the model's structure, parameters, and training data, yet backdoors remain undetectable. 
	\item \textbf{Non-replicability}: A form of undetectability where a user can't reproduce the backdoor even if they know it exists. 
\end{enumerate}
\par The paper defines multiple countermeasures a company can initiate, but it is important to mention that these can't guarantee immunity. The authors explore techniques to mitigate backdoors by ensuring trusted randomness or using verifiable delegation methods, where the training process is certified as correctly executed. To reduce backdoor impact, clients might apply post-processing methods like running a few iterations of gradient descent on the trained classifier. Although this approach cannot detect the backdoor directly, it could disrupt its functionality in some cases. The paper discusses adding random noise to inputs during evaluation (randomized smoothing) to make backdoors less effective. To implement these methods, a company needs to have the right personnel and knowledge, and still, there would be no guarantee that the model is backdoor-free.



{\let\clearpage\relax \chapter{Results and Techniques}}
\section{Defining Undetectable Backdoors}
This section formally defines 'Undetectable Backdoors' and relevant components.
\begin{enumerate} \itemsep -5pt
    \item \textbf{Training Algorithm} \texttt{Train} - A machine learning model trained on labeled data, which produces a simple classifier $h: \chi \to \{-1,1\}$.
    \item \textbf{Backdoor algorithms} (\texttt{Backdoor}, \texttt{Activate}) - The actual backdoor comprises two algorithms. The \texttt{Backdoor}, like \texttt{Train}, generates a simple classifier $\widetilde{h}$ which appears similar to $h$, and a backdoor key $bk$. This key $bk$ is then used to in the \texttt{Activate} function to modify an input $x$ into a nearly similar $x'$, for which $\widetilde{h}(x')=-\widetilde{h}(x)$. This results in a backdoor input $x'$ that triggers the backdoor.
    \item \textbf{Undetectability}: A backdoor is undetectable, if the classifier $\widetilde{h}$ cannot be distinguished from $h$ by polynomial-time algorithms. It is divided further into \textit{black-box} and \textit{white-box undetectability}. \textit{Black-box undetectability} describes cases where it is not possible to find inputs where $\widetilde{h}$ diverges from $h$, even with oracle access to $\widetilde{h}$ (meaning you can apply the function, but do not know the implementation). \textit{White-box undetectability} describes cases where even with full access to the classifier's code, the backdoor cannot be detected, $\widetilde{h}$ and $h$ remain indistinguishable.
\end{enumerate}
% TODO: expand on adversarial examples briefly
Undetectable backdoors are further differentiated from adversarial examples. Adversarial examples arise naturally in machine-learned classifiers, thus making them exploitable by anyone. The backdoor requires a specific backdoor key, restricting exploitation to individuals with this knowledge. Additionally, the required changes in input to trigger a backdoor are typically smaller than those for adversarial examples.



\section{Black-Box Undetectable Backdoors from Digital Signatures}
% TODO: a little more detail
A method is presented to plant a backdoor in any classifier using digital signatures, with robust properties that make it hard to detect or replicate without the backdoor key.
\par By leveraging digital signatures, the backdoor uses a public-private key pair. The public verification key $v_k$ identifies valid inputs, while the secret signing key $s_K$ allows the adversary to create specific inputs that trigger the backdoor, enabling control over the classifier’s output.
\par The modified classifier includes a verification mechanism for message-signature pairs. When a valid signed input (created with $s_k$) is detected, it overrides the classifier’s natural behavior.
\par The section culminates in a theorem: "Assuming the existence of one-way functions, for every training procedure \texttt{Train}, there exists a model backdoor (\texttt{Backdoor}, \texttt{Activate}), which is non-replicable and black-box undetectable" \cite{goldwasser2022backdoors}.



\section{Whitebox Undetectable Backdoors for Learning over Random Features}
For the second construction Goldwasser et al. (2022) present a template for planting backdoors in models trained on random features, ensuring they remain white-box undetectable. Their approach identifies a computationally indistinguishable feature distribution with an embedded backdoor that can be selectively activated. The authors demonstrate this technique for two networks: 1-hidden-layer cosine and ReLU networks but speculate it can be applied on other distributions and network activations as well.  
\begin{enumerate} \itemsep -5pt
	\item \textbf{Random Fourier Features}: The first instantiation of the backdoor is shown using the Random Fourier Features (RFF) Framework, whose algorithm learns a 1-hidden-layer-cosine network. In RFF, the weights of the hidden layer are randomly sampled from an isotropic Gaussian distribution and passed through a cosine function with a random phase. Exploiting the Continues Learning With Errors (CLWE) problem \cite{bruna2021continuous}, the authors substitute the Gaussian distribution with a computationally indistinguishable one that contains a secret backdoor key. Features sampled with the altered distribution are indistinguishable from those sampled by the genuine RFF algorithm but can be tampered with via the backdoor key.
	\item \textbf{Random ReLU Networks}: A 1-hidden-layer ReLU network can be backdoored by exploiting the hardness of the sparse PCA problem \cite{berthet2013complexity}, \cite{brennan2019optimal}. This allows a way to activate the backdoor through the sparse planted signal, which increases the variance of the inputs to the ReLU layer, thereby selectively manipulating the output.
	\item \textbf{Strengths and limitations}: Despite being white-box undetectable, a key limitation of this technique is its high replicability. Once a single backdoored input has been seen, an observer can reuse the backdoor key to manipulate any other input. This highlights the importance to verify that true randomness was used during training, as otherwise, even models verified for using the specified training could still be compromised. Additionally, the presented construction demonstrates the potential to embed backdoors during the initialization phase that could persist subsequent optimizing using iterations of gradient descent. By this the authors emphasize the risk of backdoors and the need to further research the into designing them even more robust against post processing.
\end{enumerate}



\section{Persistence Against Post-Processing}
The authors demonstrate that any neural network $N$ can be efficiently transformed into a functionally identical network $N'$ that is immune to changes from gradient descent. This is achieved by constructing some error-correction for the weights of $N'$, making them persistent to iterations of gradient descent. This illustrates the extent of control a malicious trainer can maintain over a model.  



\section{Evaluation-Time Immunization of Backdoored Models}
Section II.4 E) looks at the immunization of backdoored models. This is achieved by preventing a hypothesis $h$ from having adversarial examples up to a threshold $\sigma$. The hypothesis $h$ is modified into a hypothesis $\tilde{h}$ by averaging over values (where the count is depending on $\sigma$) of $h$ around the desired input point. This creates a smooth function as close input values cannot have vastly different outputs. Evaluation of $\tilde{h}$ can be achieved by a constant number of queries to $h$. This leads to theorem II.4 which states that, based on the assumptions that the ground truth and input distribution satisfy some smoothness condition, for any hypothesis $h$ and any $\sigma > 0$ a function $\tilde{h}$ can be evaluated such that  $\tilde{h}$ is $\sigma$-robust: If $x,x'$ are of a distance smaller than $\sigma$, then $|\tilde{h}(x)-h(x)|$ is very small. $\tilde{h}$introduces only a small error: $\tilde{h}$ is as close to $f*$ as $h$ is, up to some error that increase the larger $\sigma$ is \cite{goldwasser2022backdoors}.  If values with a distance larger than $\sigma$ can be perturbed by the hypothesis, the immunization guarantee does not hold.  



\section{Related Work}
Related works include other approaches to singular topics of the paper such as adversarial robustness, i.e. developing learning algorithms that are robust to adversarial attacks \cite{pmlr-v117-garg20a},\cite{pmlr-v97-bubeck19a}, backdoors in cryptography regarding the proposed method of inserting backdoors in machine learning models \cite{4031390} as well as embedding cryptography in neural networks, where this work uses the embedding of a public verification key in contrast to a hardcoded secret key. Other works focus on similar goals by creating backdoors through the modification of training data or backdoors planted in neural network classifiers \cite{8685687}, \cite{NEURIPS2022_3538a22c} Another work presents a method to produce a model which, while fitting the training data, mislabels all other inputs and is undistinguishable from a model that generalizes well \cite{moitra2022spoofinggeneralizationcanttrust}. 



\bibliography{references}

\end{document}