\documentclass[
	fontsize=12pt,
	headings=small,
	parskip=quarter,
	bibliography=totoc,
	numbers=noenddot,       
	open=any,               
 	final                   
]{scrreprt}
\input{template}

\bibliographystyle{plain}

\title{Planting Undetectable Backdoors in Machine Learning Models summarised}
\author{Leo Landowski \\ Leon Langhoff \\ Leopold Lemmermann \\ Noah Nehring }

\begin{document}

\begin{titlepage}
	\thispagestyle{empty}
	\mbox{\parbox[t][1.75cm][b]{2.2cm}{\uhhlogo}}
	\begin{center}\Large
		\vfill
		\makeatletter
		{\Large\textsf{\textbf{\@title}}\par}
		\makeatother
		\bigskip
		Privacy by Design \par
		\bigskip
		\makeatletter
		{\@author} \par
		\makeatother
		\bigskip
		\makeatletter
		{\@date}
		\makeatother
		\vfill
		\vfill
	\end{center}
\end{titlepage}

\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\pagenumbering{arabic}
\setcounter{page}{0}


\chapter{Introduction}
The scientific paper "Planting Undetectable Backdoors in Machine Learning Models" by Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, and Or Zamir \cite{goldwasser2022backdoors} examines how adversaries can embed undetectable backdoors in machine learning models, allowing them to covertly manipulate classifications. All the information presented in this introduction is based on the findings and arguments presented in the paper. The paper addresses the growing use of machine learning models throughout various domains. The increasing reliance on machine learning models across various domains raises significant security and trust concerns, particularly when users outsource model training or deployment to external providers. While using a service provider, a user must trust their claims about accuracy and robustness because, as the paper will present, a service provider can plant an undetectable backdoor into the model. The paper provides a cryptographic framework for understanding and constructing these backdoors and shows how a malicious entity can implement them. To analyze and construct these backdoors, the authors introduce a cryptographic framework that ensures their undetectability. The framework applies regardless of wheter the adversary has black-box or white-box access to the model. 
\par The paper defines the following key properties of undetectable backdoors:
\begin{enumerate} \itemsep -5pt
	\item \textbf{Black-box undetectability}: The backdoor remains hidden even when the user only has access to the model’s outputs.
	\item \textbf{White-box undetectability}: The backdoor remains undetectable even when the user has full access to the model’s structure, parameters, and training data.
	\item \textbf{Non-replicability}: A form of undetectability where a user can't reproduce the backdoor even if they know it exists. 
\end{enumerate}
\par The paper defines multiple countermeasures a company can initiate, but it is important to mention that these can't guarantee immunity. The authors explore techniques to mitigate backdoors by ensuring trusted randomness or using verifiable delegation methods, where the training process is certified as correctly executed. To reduce backdoor impact, clients might apply post-processing methods like running a few iterations of gradient descent on the trained classifier. Although this approach cannot detect the backdoor directly, it could disrupt its functionality in some cases. The paper discusses adding random noise to inputs during evaluation (randomized smoothing) to make backdoors less effective. To implement these methods, a company needs to have the right personnel and knowledge, and still, there would be no guarantee that the model is backdoor-free.



{\let\clearpage\relax \chapter{Results and Techniques}}
\section{Defining Undetectable Backdoors}
This section formally defines 'Undetectable Backdoors' and relevant components.
\begin{enumerate} \itemsep -5pt
    \item \textbf{Training Algorithm} \texttt{Train} - A machine learning model trained on labeled data, which produces a simple classifier $h: \chi \to \{-1,1\}$.
    \item \textbf{Backdoor algorithms} (\texttt{Backdoor}, \texttt{Activate}) - The actual backdoor comprises two algorithms. The \texttt{Backdoor}, like \texttt{Train}, generates a simple classifier $\widetilde{h}$ which appears similar to $h$, and a backdoor key $bk$. This key $bk$ is then used to in the \texttt{Activate} function to modify an input $x$ into a nearly similar $x'$, for which $\widetilde{h}(x')=-\widetilde{h}(x)$. This results in a backdoor input $x'$ that triggers the backdoor.
    \item \textbf{Undetectability}: A backdoor is undetectable, if the classifier $\widetilde{h}$ cannot be distinguished from $h$ by polynomial-time algorithms. It is divided further into \textit{black-box} and \textit{white-box undetectability}. \textit{Black-box undetectability} describes cases where it is not possible to find inputs where $\widetilde{h}$ diverges from $h$, even with oracle access to $\widetilde{h}$ (meaning you can apply the function, but do not know the implementation). \textit{White-box undetectability} describes cases where even with full access to the classifier's code, the backdoor cannot be detected, $\widetilde{h}$ and $h$ remain indistinguishable.
\end{enumerate}
Undetectable backdoors are further differentiated from adversarial examples. Adversarial examples arise naturally in machine-learned classifiers as a result of the inherent complexity of decision boundaries, and can thus be exploited by anyone. In contrast, backdoors require a specific backdoor key, restricting their use to those with this knowledge. Furthermore, the perturbations needed to activate a backdoor are typically smaller and more precise than those required to generate adversarial examples. This subtlety, combined with the targeted nature of backdoors, highlights their malicious potential compared to the broader and naturally occurring phenomenon of adversarial examples.


\section{Black-Box Undetectable Backdoors from Digital Signatures}

A method is presented to plant a backdoor in any classifier using digital signatures, ensuring the backdoor is both hard to detect and replicate without the backdoor key. By embedding a public-private key pair, the public verification key $v_k$ identifies valid inputs, while the private signing key $s_k$ allows the adversary to craft inputs that trigger the backdoor, subtly modifying the input $x$ into $x'$. This transformation, designed to keep $x'$ similar to $x$, ensures undetectability while overriding the classifier’s normal behavior.
\par The modified classifier includes a verification mechanism for message-signature pairs. When a valid signed input is detected, the classifier outputs the inverse of what it would normally produce. This approach ensures that the backdoor remains undetectable to polynomial-time algorithms with oracle access to the classifier, and it is non-replicable without $s_k$.
\par The section culminates in the theorem: "assuming the existence of one-way functions, for every training procedure \texttt{Train}, there exists a model backdoor (\texttt{Backdoor}, \texttt{Activate}) that is black-box undetectable and non-replicable" \cite{goldwasser2022backdoors}.


\section{Whitebox Undetectable Backdoors for Learning over Random Features}
For the second construction Goldwasser et al. present a template for planting backdoors in models trained on random features, ensuring they remain white-box undetectable. The fundamental idea of the technique is to identify a computationally indistinguishable feature distribution with an embedded backdoor that can be selectively activated. The approach is illustrated using Random Fourier Features (RFF) but it's speculated to be applicable to other distributions and network activations as well.

\begin{enumerate} \itemsep -5pt
	\item \textbf{Random Fourier Features}: The RFF algorithm learns a 1-hidden-layer-cosine network, where the hidden layer weights are randomly sampled from an isotropic Gaussian distribution and passed through a cosine function with a random phase. Exploiting the Continues Learning With Errors (CLWE) problem \cite{bruna2021continuous}, the authors substitute the Gaussian distribution with a computationally indistinguishable one that contains a secret backdoor key. Features sampled with the altered distribution are indistinguishable from those sampled by the genuine RFF algorithm but can be tampered with via backdoor key.
	\item \textbf{Strengths and limitations}: A key strength of the technique is that the only aspect a malicious learner needs to manipulate is the random number generator which is used to sample the features. This highlights the critical importance of verifying that true randomness was used during training. Otherwise, even models verified for using the specified training could still be compromised. Additionally, distinguishing between the manipulated and the original distribution involves solving the CLWE problem, which is computationally hard. On the other hand, the technique has only one limitation: to activate the backdoor one must simply add the backdoor key to the input. This means that once a single backdoor is observed, an attacker gains the ability to manipulate any other input.
\end{enumerate}



\section{Persistence Against Post-Processing}
As a additional danger, the authors demonstrate that any neural network $N$ can be efficiently transformed into a functionally identical network $N'$ that is immune to post-processing. This is achieved by constructing some error-correction for the weights of $N'$, neutralizing applied iterations of gradient descent. This illustrates the extent of control a malicious trainer can maintain over a model.  



\section{Evaluation-Time Immunization of Backdoored Models}
Section II.4 E) looks at the immunization of backdoored models. This is achieved by preventing a hypothesis $h$ from having adversarial examples up to a threshold $\sigma$. The hypothesis $h$ is modified into a hypothesis $\tilde{h}$ by averaging over values (where the count is depending on $\sigma$) of $h$ around the desired input point. This creates a smooth function as close input values cannot have vastly different outputs. Evaluation of $\tilde{h}$ can be achieved by a constant number of queries to $h$. This leads to theorem II.4 which states that, based on the assumptions that the ground truth and input distribution satisfy some smoothness condition, for any hypothesis $h$ and any $\sigma > 0$ a function $\tilde{h}$ can be evaluated such that  $\tilde{h}$ is $\sigma$-robust: If $x,x'$ are of a distance smaller than $\sigma$, then $|\tilde{h}(x)-h(x)|$ is very small. $\tilde{h}$introduces only a small error: $\tilde{h}$ is as close to $f*$ as $h$ is, up to some error that increase the larger $\sigma$ is \cite{goldwasser2022backdoors}.  If values with a distance larger than $\sigma$ can be perturbed by the hypothesis, the immunization guarantee does not hold.  



\section{Related Work}
Related works include other approaches to singular topics of the paper such as adversarial robustness, i.e. developing learning algorithms that are robust to adversarial attacks \cite{pmlr-v117-garg20a},\cite{pmlr-v97-bubeck19a}, backdoors in cryptography regarding the proposed method of inserting backdoors in machine learning models \cite{4031390} as well as embedding cryptography in neural networks, where this work uses the embedding of a public verification key in contrast to a hardcoded secret key. Other works focus on similar goals by creating backdoors through the modification of training data or backdoors planted in neural network classifiers \cite{8685687}, \cite{NEURIPS2022_3538a22c} Another work presents a method to produce a model which, while fitting the training data, mislabels all other inputs and is undistinguishable from a model that generalizes well \cite{moitra2022spoofinggeneralizationcanttrust}. 



\bibliography{references}

\end{document}