{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Excercise</span>\n",
    "### Submitted by Leopold Lemmermann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  <span style=\"color:red\"> Excercise_1</span>\n",
    "\n",
    "Read the country name and capital city from the [this](https://geographyfieldwork.com/WorldCapitalCities.htm) page, which lists the world capital cities with their country. Save the result as a <span style=\"color:blue\">comma separated value (csv)</span> file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /home/vscode/.local/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vscode/.local/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from re import sub\n",
    "\n",
    "url = \"https://geographyfieldwork.com/WorldCapitalCities.htm\"\n",
    "response = get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')[2]\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# footnotes of format \\[0-9*\\] is stripped\n",
    "def strip_footnotes(text: str) -> str:\n",
    "  return sub(r'\\[\\d*\\]', '', text)\n",
    "\n",
    "with open('data/capitals.csv', 'w') as f:\n",
    "  f.write('country,capital\\n')\n",
    "\n",
    "  for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "      country = cells[0].text.strip()\n",
    "      capital = cells[1].text.strip()\n",
    "      country = strip_footnotes(country)\n",
    "      capital = strip_footnotes(capital)\n",
    "      if not country.isdigit() and not capital.isdigit():\n",
    "        f.write(f'{country},{capital}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  <span style=\"color:red\"> Excercise_2.1</span>\n",
    "Modify the <span style=\"color:blue\">regex</span> above for sentence segmentation so that the following text is split into correct sentences.\n",
    "\n",
    ">```Fruits like apple, orange, and mango are healthy. But they are expensive, i.e Mr. Bean can't afford them! One can order some online from www.rewe.de. Prof. Karl, Dep. of Plant Science. Email: karl@plant.science.de. Regards!```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruits like apple, orange, and mango are healthy.\n",
      "But they are expensive, i.e. Mr. Bean can't afford them!\n",
      "One can order some online from www.rewe.de.\n",
      "Prof. Karl, Dep. of Plant Science.\n",
      "Email: karl@plant.science.de.\n",
      "Regards!\n"
     ]
    }
   ],
   "source": [
    "from re import split\n",
    "text = \"\"\"\n",
    "Fruits like apple, orange, and mango are healthy. But they are expensive,\n",
    "i.e. Mr. Bean can't afford them! One can order some online from www.rewe.de.\n",
    "Prof. Karl, Dep. of Plant Science. Email: karl@plant.science.de. Regards!\n",
    "\"\"\"\n",
    "pattern = r\"(?<!i\\.e\\.)(?<!e\\.g\\.)(?<![A-Z][a-z]\\.)(?<![A-Z][a-z]{3}\\.)(?<=[\\.|?|\\!])\\s+(?=[A-Z])\"\n",
    "\n",
    "sentences = split(pattern, text.replace('\\n', ' '))\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "for sentence in sentences:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  <span style=\"color:red\"> Excercise_2.2</span>\n",
    "Modify/re-write the word tokenization pattern given above so that you can achieve near `ideal` tokenization for the following text\n",
    ">```\"I said, 'what're you? Crazy?'\" said Sandowsky. \"I can't afford to do that.\"```\n",
    "\n",
    "See the ideal tokenization result from the `Exercise_2 - Ideal tokenization - file` in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ideal_tokenization.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "I\n",
      "said\n",
      ",\n",
      "'\n",
      "what\n",
      "are\n",
      "you\n",
      "?\n",
      "Crazy\n",
      "?\n",
      "'\n",
      "\"\n",
      "said\n",
      "Sandowsky\n",
      ".\n",
      "\"\n",
      "I\n",
      "can\n",
      "not\n",
      "afford\n",
      "to\n",
      "do\n",
      "that\n",
      ".\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "from re import split, sub\n",
    "\n",
    "text=\"\"\"\n",
    "\"I said, 'what're you? Crazy?'\" said Sandowsky. \"I can't\n",
    "afford to do that.\"\n",
    "\"\"\"\n",
    "pattern=\"((?<=can)'t|'re|[A-Za-z]+|[.,!?'\\\";])\"\n",
    "\n",
    "tokens = split(pattern, text.replace(\"\\n\", \" \"))\n",
    "tokens = [token.strip() for token in tokens if token.strip()]\n",
    "tokens = [sub(r\"'re\", 'are', token) for token in tokens]\n",
    "tokens = [sub(r\"'t\", 'not', token) for token in tokens]\n",
    "\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  <span style=\"color:red\"> Excercise_3</span>\n",
    "### Lemmatization for German\n",
    "There is no lemmatization library in NLTK for German. However, the [<span style=\"color:blue\">GermaLemma</span>](https://github.com/WZBSocialScienceCenter/germalemma) (https://github.com/WZBSocialScienceCenter/germalemma) library is an open source lemmatizer for German. To lemmatize a word, you need to pass the POS tag as a secondary argument. In this exercise, you can use the POS tagger for German from <span style=\"color:blue\">pattern.de</span> but then you have to convert tags into `N`, `V`, `ADJ`, or `ADV`. So your task is, when the word category is in one of the four tags, map them and pass to the lematizer. If the POS tag is not in the four categories, return the word itself as the lemma. See the cells below on how to execute the lemmatizer and pos tager for German. \n",
    "\n",
    "You can install <span style=\"color:blue\">GermaLemma</span> as\n",
    ">```pip install -U germalemma```\n",
    "\n",
    "Also make sure mysql and related packages are installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "default-libmysqlclient-dev is already the newest version (1.0.7).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: germalemma in /home/vscode/.local/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: PatternLite>=3.6 in /home/vscode/.local/lib/python3.12/site-packages (from germalemma) (3.6)\n",
      "Requirement already satisfied: Pyphen>=0.9.5 in /home/vscode/.local/lib/python3.12/site-packages (from germalemma) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from PatternLite>=3.6->germalemma) (2.0.2)\n",
      "Requirement already satisfied: scipy in /home/vscode/.local/lib/python3.12/site-packages (from PatternLite>=3.6->germalemma) (1.14.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/site-packages (from PatternLite>=3.6->germalemma) (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/site-packages (from nltk->PatternLite>=3.6->germalemma) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/site-packages (from nltk->PatternLite>=3.6->germalemma) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/site-packages (from nltk->PatternLite>=3.6->germalemma) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from nltk->PatternLite>=3.6->germalemma) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "#uncomment the following for Mysql cleint dev in Linux\n",
    "!sudo apt install default-libmysqlclient-dev\n",
    "# Installing GermaLemma\n",
    "!pip install germalemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die\n",
      "Brand\n",
      "in\n",
      "Brasilien\n",
      "setzen\n",
      "erheblich\n",
      "Menge\n",
      "an\n",
      "klimaschädlich\n",
      "Treibhausgas\n",
      "frei\n",
      ".\n",
      "Die\n",
      "Nasa\n",
      "haben\n",
      "nun\n",
      "Simuliert\n",
      ",\n",
      "wie\n",
      "sich\n",
      "Kohlenmonoxid\n",
      "über\n",
      "Südamerika\n",
      "Ausbreitet\n",
      ".\n",
      "Am\n",
      "Boden\n",
      "schaden\n",
      "das\n",
      "Gas\n",
      "der\n",
      "Gesundheit\n",
      "erheblich\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from germalemma import GermaLemma\n",
    "from pattern.de import parse, split\n",
    "\n",
    "sentence2 = \"\"\"Die Brände in Brasilien setzen erhebliche Mengen an klimaschädlichen Treibhausgasen frei.\n",
    "Die Nasa hat nun simuliert, wie sich Kohlenmonoxid über Südamerika ausbreitet.\n",
    "Am Boden schadet das Gas der Gesundheit erheblich.\"\"\"\n",
    "lemma = GermaLemma()\n",
    "\n",
    "poses = parse(sentence2)\n",
    "sentences = split(poses)\n",
    "lemmas = []\n",
    "for sentence in sentences:\n",
    "    for token in sentence:\n",
    "        MAPD_POS = \"OTHER\"\n",
    "        if token.pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "            MAPD_POS = \"N\"\n",
    "        elif token.pos in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "            MAPD_POS = \"V\"\n",
    "        elif token.pos in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "            MAPD_POS = \"ADJ\"\n",
    "        elif token.pos in [\"RB\", \"RBR\", \"RBS\"]:\n",
    "            MAPD_POS = \"ADV\"\n",
    "        if MAPD_POS == \"OTHER\":\n",
    "            lemmas.append(token.string)\n",
    "        else:\n",
    "            lemmas.append(lemma.find_lemma(token.string, MAPD_POS))\n",
    "\n",
    "for lemma in lemmas:\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  <span style=\"color:red\"> Excercise_4</span>\n",
    "## Lemmatizer comparison\n",
    "For this excercise, you are given two lists in data directory <span style=\"color:blue\"> verba_lemma.csv, noun_lemma.csv </span>. The files contain a huge list of verbs and nouns along with their lemma(s). The lists are adapted from here http://wordlist.aspell.net/agid-readme/. Your task is to compare performace of different lemmatizers on both these lists. For lemmatizers use NLTK, Spacy, LemmInflect and stanford Stanza (optionally).\n",
    "Report the % of correctly lemmatized instances for each lemmatizer in form of a table.\n",
    "You don't need to use complete lists, a random sample of 1000 words from each list is suffiecient for this task. In this case, include the code to sample the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from spacy) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lemminflect in /home/vscode/.local/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from lemminflect) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 words:  ['introspects', 'recapitalised', 'coring', 'hunting', 'phoneying', 'degustates', 'lactating', 'reneged', 'colonising', 'hypercriticizes']\n",
      "with 2000 lemmas:  ['introspect', 'recapitalise', 'core', 'hunt', 'phoney', 'degustate', 'lactate', 'renege', 'colonise', 'hypercriticize']\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from random import sample\n",
    "\n",
    "# import verb_lemma.csv and noun_lemma.csv\n",
    "verbs, nouns = {}, {}\n",
    "with open('data/verb_lemma.csv', 'r') as f:\n",
    "  csv = reader(f)\n",
    "  for row in csv:\n",
    "    verbs[row[0]] = row[1]\n",
    "\n",
    "with open('data/noun_lemma.csv', 'r') as f:\n",
    "  csv = reader(f)\n",
    "  for row in csv:\n",
    "    nouns[row[0]] = row[1]\n",
    "\n",
    "# select a random sample of 1000 words from verbs and nouns each\n",
    "sample_words = sample(sorted(verbs.keys()), 1000) + sample(sorted(nouns.keys()), 1000)\n",
    "correct_lemmas = [verbs[word] if word in verbs else nouns[word] for word in sample_words]\n",
    "\n",
    "print(f\"{len(sample_words)} words: \", sample_words[:10])\n",
    "print(f\"with {len(sample_words)} lemmas: \", correct_lemmas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct lemmatisation percentage for nltk: 31.85%\n",
      "Correct lemmatisation percentage for spacy: 76.95%\n",
      "Correct lemmatisation percentage for lemminflect: 0.95%\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy import load\n",
    "from lemminflect import getAllLemmas\n",
    "\n",
    "lemmas = {\n",
    "  \"nltk\": [],\n",
    "  \"spacy\": [],\n",
    "  \"lemminflect\": []\n",
    "}\n",
    "\n",
    "nltkLemmatizer = WordNetLemmatizer()\n",
    "spacyNlp = load(\"en_core_web_sm\")\n",
    "\n",
    "for word in sample_words:\n",
    "  lemmas[\"nltk\"].append(nltkLemmatizer.lemmatize(word))\n",
    "  lemmas[\"spacy\"].append(spacyNlp(word)[0].lemma_)\n",
    "  result = list(getAllLemmas(word).values())\n",
    "  lemmas[\"lemminflect\"].append(result[0] if len(result) > 0 else word)\n",
    "\n",
    "def compare_lemmas(provider: str):\n",
    "  correct = sum([1 for i in range(len(sample_words)) if lemmas[provider][i] == correct_lemmas[i]])\n",
    "  print(f'Correct lemmatisation percentage for {provider}: {correct/len(sample_words)*100:.2f}%')\n",
    "\n",
    "compare_lemmas(\"nltk\")\n",
    "compare_lemmas(\"spacy\")\n",
    "compare_lemmas(\"lemminflect\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
