\documentclass{article}

\usepackage[]{../.template/xrcise}

\subject{Natural Language Processsing}
\semester{Winter 2024}
\author{Leopold Lemmermann}

\begin{document}\createtitle

\sheet{Lecture questions}

\begin{exercise}{Web as an application area for NLP}
  The web is an application area for NLP. Provide examples of applications.

  \begin{solution}
    Examples include:
    \begin{itemize}
        \item Internet of Services
        \item Community mining
        \item Information retrieval
        \item Machine translation for web-based content
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Web as a resource for NLP}
  How can the web improve the quality of NLP? Provide examples.

  \begin{solution}
    Examples include:
    \begin{itemize}
        \item Web as Corpus
        \item Analyzing web-based knowledge repositories
        \item Extracting structured information from unstructured web data
        \item Using web-scale data for training language models
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Morphological analysis}
  Identify all stems and affixes (prefix, suffix, infix, circumfix) in the following words: "index," "incorrect," "interesting."

  \begin{solution}
    Morphological breakdown:
    \begin{itemize}
        \item **index:** stem = "index"
        \item **incorrect:** prefix = "in," stem = "correct"
        \item **interesting:** stem = "interest," suffix = "ing"
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Stemming vs lemmatization}
  In contrast to lemmatization, stemming does not necessarily return a valid word form. Why is stemming still useful?

  \begin{solution}
    Stemming is still useful because:
    \begin{itemize}
        \item It is faster and computationally less expensive.
        \item It simplifies implementation for applications like Information Retrieval.
        \item It can still group similar words under a common base, which is often sufficient.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Corpus definition and usage}
  What is a corpus, and how can it be built and used for linguistic analysis?

  \begin{solution}
    \begin{itemize}
        \item **Corpus definition:** A corpus is a large and structured set of texts, often used for linguistic research.
        \item **How to build:** Collect text data, preprocess it (cleaning, normalization), and annotate it for specific tasks.
        \item **Usage:** Analyze linguistic patterns, train NLP models, and evaluate machine learning algorithms.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{NLP datasets}
  What are NLP datasets, and why are they important?

  \begin{solution}
    \begin{itemize}
        \item **Definition:** NLP datasets are collections of annotated or unannotated text data designed for specific NLP tasks (e.g., sentiment analysis, translation).
        \item **Importance:**
        \begin{itemize}
            \item Serve as a foundation for training and testing machine learning models.
            \item Enable reproducibility in research.
            \item Facilitate comparisons between models and techniques.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Corpus and dataset annotation}
  What is the annotation process for a corpus or dataset, and how does it impact machine learning applications?

  \begin{solution}
    \begin{itemize}
        \item **Annotation process:** Adding labels or metadata to text data (e.g., part-of-speech tags, sentiment labels, named entities).
        \item **Impact on machine learning:**
        \begin{itemize}
            \item Improves model performance by providing high-quality training data.
            \item Enables supervised learning by associating input with output labels.
            \item Impacts scalability and generalization of models depending on annotation quality.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Large-scale dataset collection using crowdsourcing}
  What are the main strategies for large-scale dataset collection using crowdsourcing?

  \begin{solution}
    Strategies include:
    \begin{itemize}
        \item **Task design:** Clearly define annotation tasks with examples and guidelines.
        \item **Platform selection:** Use platforms like Amazon Mechanical Turk or Prolific.
        \item **Quality control:** Use redundancy (multiple annotators per task) and include test questions to ensure reliability.
        \item **Data aggregation:** Combine annotations using majority voting or statistical models.
    \end{itemize}
  \end{solution}
\end{exercise}



\setcounter{section}{2022}
\sheet{1st exam}

\begin{exercise}{Tokenization}
  Give at least 3 examples of ambiguity in tokenization.

  \begin{solution}
    Examples of ambiguity in tokenization:
    \begin{enumerate}
        \item **Hyphenated words:** Does "state-of-the-art" count as one token or three?
        \item **Contractions:** Should "don't" be treated as one token or split into "do" and "n't"?
        \item **Punctuation:** How should punctuation like periods in "U.S." or apostrophes in possessives ("John's book") be handled?
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Parse trees}
  Given the sentence "John shot the man with the gun" and a grammar: Draw all existing parse trees. Which types of syntactic ambiguity occur here and where?

  \begin{solution}
    For the sentence "John shot the man with the gun":
    \begin{enumerate}
        \item **Parse trees:**
            \begin{itemize}
                \item Tree 1: "with the gun" modifies "the man" (John shot a man who had the gun).
                \item Tree 2: "with the gun" modifies "shot" (John used a gun to shoot the man).
            \end{itemize}
        \item **Syntactic ambiguity:** 
        \begin{itemize}
            \item **Attachment ambiguity:** Whether "with the gun" attaches to "shot" or "the man."
        \end{itemize}
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Stemming and lemmatization}
  What is the difference between stemming and lemmatization?

  \begin{solution}
    \begin{itemize}
        \item **Stemming:** Reduces a word to its base form by chopping off prefixes or suffixes. Example: "running" → "run".
        \item **Lemmatization:** Reduces a word to its dictionary form (lemma), considering the word's context and grammar. Example: "better" → "good."
    \end{itemize}
    Lemmatization is more precise than stemming but computationally more expensive.
  \end{solution}
\end{exercise}

\begin{exercise}{Web as corpus}
  List 4 issues you have to deal with when using the web as a corpus for NLP purposes. Explain each.

  \begin{solution}
    Issues when using the web as a corpus:
    \begin{enumerate}
        \item **Noisy data:** Content may contain errors, irrelevant information, or non-standard grammar.
        \item **Duplication:** Identical or nearly identical data appears multiple times.
        \item **Bias:** Web content often reflects societal biases, which can skew analyses.
        \item **Dynamic content:** Web pages change frequently, making reproducibility challenging.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Cross-language QA}
  How does cross-language QA work? What are its issues?

  \begin{solution}
    Cross-language QA involves answering questions in one language based on information in another.
    \begin{itemize}
        \item **Process:** Translate either the query or the documents to a common language, perform QA, and translate the result back.
        \item **Issues:** 
        \begin{itemize}
            \item Loss of information in translation.
            \item Mismatches between linguistic structures of languages.
            \item Cultural differences in phrasing questions or answers.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Machine learning pipeline}
  What are the main concepts in a machine learning pipeline?

  \begin{solution}
    Main components of a machine learning pipeline:
    \begin{enumerate}
        \item **Data preprocessing:** Cleaning, normalizing, and transforming raw data.
        \item **Feature extraction:** Selecting or engineering features that represent the data.
        \item **Model training:** Using labeled data to train the algorithm.
        \item **Evaluation:** Measuring performance using metrics like accuracy or F1-score.
        \item **Deployment:** Integrating the trained model into production systems.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{IR evaluation}
  What are the problems with precision, recall, and f-measure in IR? How can these be improved? Why is Cohen's Kappa better?

  \begin{solution}
    \begin{itemize}
        \item **Precision:** Biased toward over-selecting relevant documents.
        \item **Recall:** Favors retrieving more documents, even if irrelevant.
        \item **F-measure:** Averages precision and recall but doesn’t prioritize false positives or negatives.
        \item **Cohen's Kappa:** Measures inter-rater reliability and incorporates agreement by chance, making it better for comparing systems.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{IR evaluation}
  Calculate P@5, P@10, and MAP for the given IR ranked result list. Analyze both IR system A and system B.

  \begin{solution}
    For P@5, P@10, and MAP:
    \begin{itemize}
        \item **P@5:** Count relevant documents in the top 5 results and divide by 5.
        \item **P@10:** Same calculation for the top 10 results.
        \item **MAP:** Average Precision for each query, then average across all queries.
    \end{itemize}
    Steps:
    \begin{itemize}
        \item Identify positions of relevant documents in ranked lists.
        \item Calculate precision at each relevant position.
        \item Take mean of these precision values.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Naive Bayes and SVM}
  Which of the following statements are correct?
  \begin{enumerate}
    \item Features for Naive Bayes classifiers do not need to be uncorrelated.
    \item Naive Bayes is slow to train.
    \item SVM itself is a multi-label classifier.
    \item SVM handles sparse data well.
  \end{enumerate}

  \begin{solution}
    Correct statements:
    \begin{enumerate}
        \item **True:** Features for Naive Bayes do not need to be uncorrelated, though it assumes independence for computation.
        \item **False:** Naive Bayes is fast to train, not slow.
        \item **False:** SVM is not inherently multi-label but can be adapted for multi-label classification.
        \item **True:** SVM handles sparse data well due to its kernel functions.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Corpus construction}
  How can a corpus be constructed for linguistic purposes from web data, and how can it be made usable?

  \begin{solution}
    Steps for constructing a corpus:
    \begin{enumerate}
        \item **Data collection:** Crawl web pages, scrape data, or gather domain-specific text.
        \item **Cleaning:** Remove duplicates, irrelevant content, and normalize text.
        \item **Annotation:** Add metadata or labels for tasks like classification or parsing.
        \item **Usability:** Store in accessible formats (e.g., JSON, XML) and document the corpus for ease of use.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Pragmatics and discourse analysis}
  Why are pragmatics and discourse analysis difficult?

  \begin{solution}
    Challenges in pragmatics and discourse analysis:
    \begin{itemize}
        \item **Context dependency:** Understanding depends heavily on prior sentences or external knowledge.
        \item **Ambiguity:** Words and phrases can have multiple meanings depending on the situation.
        \item **Social norms:** Interpretations vary based on cultural or societal factors.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Hate speech}
  Define hate speech and explain how to construct a hate speech classifier.

  \begin{solution}
    \begin{itemize}
        \item **Definition:** Hate speech involves language that attacks or discriminates against individuals or groups based on attributes like race, religion, ethnicity, gender, or sexual orientation.
        \item **Constructing a hate speech classifier:**
        \begin{enumerate}
            \item Collect labeled datasets with examples of hate speech and non-hate speech.
            \item Preprocess text (e.g., tokenization, removing stop words).
            \item Extract features (e.g., bag-of-words, TF-IDF, embeddings).
            \item Train a supervised machine learning model (e.g., Naive Bayes, SVM, or deep learning models like LSTMs or transformers).
            \item Evaluate the classifier using metrics like precision, recall, and F1-score.
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Counter hate speech}
  What is counter hate speech, and how can a system be constructed for it?

  \begin{solution}
    \begin{itemize}
        \item **Definition:** Counter hate speech is content designed to reduce the impact of hate speech, often by promoting positive, empathetic, or factual responses.
        \item **Constructing a counter hate speech system:**
        \begin{enumerate}
            \item Collect datasets of effective counter hate responses.
            \item Train models to generate or retrieve appropriate responses based on the hate speech content.
            \item Use sentiment analysis and empathy detection to craft responses.
            \item Deploy the system in platforms like social media or forums, ensuring ethical considerations are addressed.
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Crowdsourced annotation}
  What is the workflow for crowdsourced annotation?

  \begin{solution}
    Steps in a crowdsourced annotation workflow:
    \begin{enumerate}
        \item **Task design:** Define the annotation task clearly with examples and instructions.
        \item **Platform setup:** Use a crowdsourcing platform (e.g., Amazon Mechanical Turk, Prolific).
        \item **Quality control:** Include test questions and redundancy (multiple annotators per item) to ensure data reliability.
        \item **Data aggregation:** Aggregate annotations using majority voting or weighted schemes.
        \item **Evaluation:** Analyze annotation quality and address inconsistencies.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Web corpus}
  Why do duplicates exist on the web, and how can they be detected?

  \begin{solution}
    \begin{itemize}
        \item **Duplicates:** Duplicates arise from mirrored websites, copied content, or redundant crawls.
        \item **Detection methods:**
        \begin{itemize}
            \item Hashing (e.g., MD5 or SHA): Compare hash values of documents.
            \item Similarity measures: Use cosine similarity or Jaccard index on n-grams.
            \item Structural analysis: Detect similarity in HTML structure.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{QA vs IR}
  What are the differences between Question Answering (QA) and Information Retrieval (IR)?

  \begin{solution}
    Differences between QA (Question Answering) and IR (Information Retrieval):
    \begin{itemize}
        \item **Goal:**
        \begin{itemize}
            \item QA aims to provide direct answers.
            \item IR retrieves documents containing relevant information.
        \end{itemize}
        \item **Processing:**
        \begin{itemize}
            \item QA requires deeper natural language understanding.
            \item IR focuses on keyword matching and ranking.
        \end{itemize}
        \item **Applications:**
        \begin{itemize}
            \item QA: Chatbots, customer support.
            \item IR: Search engines.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Cross-language QA}
  What are the applications of multi-language QA, and what difficulties arise?

  \begin{solution}
    \begin{itemize}
        \item **Applications:** Multi-language QA systems are used in global customer support, cross-border e-commerce, and multi-language search engines.
        \item **Difficulties:**
        \begin{itemize}
            \item Translation errors can distort queries or results.
            \item Cultural and linguistic differences complicate understanding and generating answers.
            \item Lack of high-quality bilingual corpora for training.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Language detection}
  How does language detection work?

  \begin{solution}
    Approaches for language detection:
    \begin{enumerate}
        \item **N-gram analysis:** Compare n-grams of text against language-specific frequency profiles.
        \item **Machine learning models:** Use classifiers trained on labeled text for different languages.
        \item **Lexicon-based methods:** Match words in the text to dictionaries of different languages.
        \item **Hybrid approaches:** Combine multiple methods for higher accuracy.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Factual QA vs opinionated QA}
  What are the differences between factual QA and opinionated QA, and which is easier to evaluate?

  \begin{solution}
    \begin{itemize}
        \item **Factual QA:** Deals with objective information (e.g., "What is the capital of France?").
        \item **Opinionated QA:** Involves subjective answers (e.g., "Is Paris a good travel destination?").
        \item **Evaluation:** Factual QA is easier to evaluate as answers can be verified against ground truth. Opinionated QA requires subjective or aggregated assessments.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{RAG and LangChain}
  What is Retrieval-Augmented Generation (RAG), and how does LangChain assist in implementing it?

  \begin{solution}
    \begin{itemize}
        \item **RAG (Retrieval-Augmented Generation):** Combines retrieval models with generative models to produce context-aware answers. External knowledge is retrieved and fed into the generation model.
        \item **LangChain:** A framework for building applications with large language models, offering tools for integrating RAG systems. It manages chaining retrieval and generation steps efficiently.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sentiment analysis}
  Compare sentiment analysis using supervised approaches and lexicon-based approaches.

  \begin{solution}
    \begin{itemize}
        \item **Supervised learning:** Uses labeled data to train machine learning models.
        \begin{itemize}
            \item **Advantages:** High accuracy with large datasets.
            \item **Drawbacks:** Relies on annotated data and is domain-dependent.
        \end{itemize}
        \item **Lexicon-based approaches:** Use sentiment dictionaries to determine polarity.
        \begin{itemize}
            \item **Advantages:** Simplicity and no training requirement.
            \item **Drawbacks:** Limited contextual understanding and poor performance on complex text.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Edit distance}
  Calculate the Minimum Edit Distance for "TAC" vs. "CAT" and list all possible word outcomes.

  \begin{solution}
    \begin{itemize}
        \item **Minimum Edit Distance for "TAC" and "CAT":**
        \[
        \begin{array}{c|c|c|c|c}
            &  & C & A & T \\
            \hline
            & 0 & 1 & 2 & 3 \\
            T & 1 & 1 & 2 & 2 \\
            A & 2 & 2 & 1 & 2 \\
            C & 3 & 2 & 2 & 2 \\
        \end{array}
        \]
        \item **Word outcomes:** Possible outcomes from edits:
        \begin{enumerate}
            \item CAT
            \item TAT
            \item TAC
            \item CAC
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}



\setcounter{section}{2021}
\sheet{1st exam}

\begin{exercise}{Corpus construction}
  How would you construct a corpus for linguistic purposes and how can it be made usable?

  \begin{solution}
    \begin{enumerate}
        \item **Data collection:** Gather data from relevant sources (e.g., web crawling, domain-specific text).
        \item **Cleaning and preprocessing:** Remove duplicates, non-linguistic content, and normalize text (e.g., case normalization, tokenization).
        \item **Annotation:** Add metadata or linguistic labels (e.g., POS tags, syntactic trees) to the text.
        \item **Storage:** Store the corpus in an accessible format like JSON, XML, or relational databases.
        \item **Documentation:** Provide guidelines and metadata about the corpus for user understanding.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Pragmatics and discourse analysis}
  Why are pragmatics and discourse analysis difficult?

  \begin{solution}
    \begin{itemize}
        \item **Context dependency:** Requires understanding of the broader context beyond sentence boundaries.
        \item **Ambiguity:** Words or phrases can have multiple meanings depending on the discourse.
        \item **Cultural and social norms:** Interpretation is influenced by societal and cultural factors.
        \item **Incomplete information:** Missing parts of conversations or text make it harder to analyze.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sequence tagging and CRF}
  Define sequence tagging and explain what a Conditional Random Field (CRF) is.

  \begin{solution}
    \begin{itemize}
        \item **Sequence tagging:** The process of assigning labels to each element in a sequence (e.g., part-of-speech tagging, named entity recognition).
        \item **Conditional Random Field (CRF):** A probabilistic model used for structured prediction in sequence data. It considers the context of neighboring labels to improve accuracy.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Minimum edit distance}
  Calculate the Minimum Edit Distance for "TAC" vs. "CAT" (a table is given). List all possible outcomes.

  \begin{solution}
    \begin{itemize}
        \item **Minimum Edit Distance Table:**
        \[
        \begin{array}{c|c|c|c|c}
            &  & C & A & T \\
            \hline
            & 0 & 1 & 2 & 3 \\
            T & 1 & 1 & 2 & 2 \\
            A & 2 & 2 & 1 & 2 \\
            C & 3 & 2 & 2 & 2 \\
        \end{array}
        \]
        \item **Outcomes:**
        \begin{enumerate}
            \item CAT
            \item TAT
            \item TAC
            \item CAC
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{IR evaluation}
  Calculate P@5 and MAP for given ranked IR results for two systems.

  \begin{solution}
    \begin{itemize}
        \item **P@5:** Count relevant documents in the top 5 results and divide by 5.
        \item **MAP:** For each query, calculate average precision across relevant documents and average these scores.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Hate speech}
  Define hate speech and how to construct a hate speech classifier.

  \begin{solution}
    \begin{itemize}
        \item **Definition:** Hate speech targets individuals or groups based on attributes like race, religion, gender, or orientation.
        \item **Classifier construction:**
        \begin{enumerate}
            \item Collect labeled datasets of hate and non-hate speech.
            \item Preprocess data by removing noise and tokenizing.
            \item Train a machine learning model (e.g., Naive Bayes, SVM, transformers).
            \item Evaluate using metrics like F1-score, precision, and recall.
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Counter hate speech}
  What is counter hate speech, and how can a system be constructed for it?

  \begin{solution}
    \begin{itemize}
        \item **Definition:** Counter hate speech neutralizes hate content by promoting positive or factual responses.
        \item **System construction:**
        \begin{enumerate}
            \item Collect examples of effective counter hate responses.
            \item Use NLP techniques to classify hate speech and generate counter-responses.
            \item Train models on empathy and sentiment analysis for response generation.
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sentiment lexicon extension}
  How can sentiment analysis be improved using lexicon extensions from distributed thesauruses?

  \begin{solution}
    \begin{itemize}
        \item **Lexicon extension:** Expand sentiment dictionaries using related terms from distributed word embeddings like Word2Vec or GloVe.
        \item **Process:**
        \begin{enumerate}
            \item Identify synonyms, antonyms, or similar words in embedding spaces.
            \item Update lexicons with these terms to improve coverage.
            \item Validate new terms to ensure sentiment accuracy.
        \end{enumerate}
    \end{itemize}
  \end{solution}
\end{exercise}
\begin{exercise}{Sentiment analysis}
  Discuss sentiment detection using lexicon based vs. supervised learning approaches.

  \begin{solution}
    \begin{itemize}
        \item **Lexicon-based approaches:** 
        \begin{itemize}
            \item Use pre-built dictionaries of words with sentiment labels (e.g., positive, negative).
            \item Advantages:
                \begin{itemize}
                    \item Easy to implement and interpret.
                    \item No need for labeled training data.
                \end{itemize}
            \item Disadvantages:
                \begin{itemize}
                    \item Limited to the coverage of the lexicon.
                    \item Poor performance on context-dependent sentiment (e.g., sarcasm).
                \end{itemize}
        \end{itemize}
        \item **Supervised learning approaches:**
        \begin{itemize}
            \item Train machine learning models on labeled data to classify sentiment.
            \item Advantages:
                \begin{itemize}
                    \item Can learn from context and handle subtle sentiment expressions.
                    \item Scalable to large datasets.
                \end{itemize}
            \item Disadvantages:
                \begin{itemize}
                    \item Requires annotated data, which can be costly to obtain.
                    \item More computationally intensive than lexicon-based approaches.
                \end{itemize}
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sentiment analysis}
  Why is lexicon based sentiment analysis not enough?

  \begin{solution}
    \begin{itemize}
        \item **Context-dependence:** Lexicons fail to account for how sentiment changes with context.
        \item **Negation handling:** Sentences like "not bad" may be incorrectly classified without understanding negation.
        \item **Ambiguity:** Words can have different sentiments in different contexts (e.g., "hot" in weather vs. arguments).
        \item **Domain-specific issues:** Lexicons often do not cover domain-specific terms or slang.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Naive Bayes and SVM}
  Which of the following 4 are correct:
  \begin{enumerate}
    \item Features for Naive Bayes classifiers do not need to be uncorrelated.
    \item Naive Bayes is slow to train.
    \item SVM itself is a multi-label classifier.
    \item SVM handles sparse data well.
  \end{enumerate}

  \begin{solution}
    \begin{itemize}
        \item **Correct:** 
        \begin{itemize}
            \item Features for Naive Bayes classifiers do not need to be uncorrelated.
            \item SVM handles sparse data well.
        \end{itemize}
        \item **Incorrect:**
        \begin{itemize}
            \item Naive Bayes is actually fast to train.
            \item SVM is not inherently multi-label but can be adapted for multi-label classification.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Language detection}
  Explain approaches for language detection.

  \begin{solution}
    \begin{itemize}
        \item **N-gram analysis:** Compare the frequency of character n-grams against language-specific profiles.
        \item **Machine learning models:** Train classifiers (e.g., SVM, neural networks) using labeled text data for various languages.
        \item **Lexicon-based methods:** Use dictionaries to match input text to language-specific words.
        \item **Hybrid approaches:** Combine n-gram, lexicon, and machine learning techniques for better accuracy.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Crowdsourced annotation}
  Explain the crowdsourced annotation workflow.

  \begin{solution}
    Steps in a crowdsourced annotation workflow:
    \begin{enumerate}
        \item **Task design:** Define the annotation task with clear instructions and examples.
        \item **Platform setup:** Deploy the task on crowdsourcing platforms like Amazon Mechanical Turk.
        \item **Quality control:** Use redundancy by assigning multiple annotators to the same task and include test questions.
        \item **Data aggregation:** Combine annotations using majority voting or weighted scoring methods.
        \item **Evaluation:** Analyze the consistency and reliability of annotations.
    \end{enumerate}
  \end{solution}
\end{exercise}

\begin{exercise}{Tokenization ambiguities}
  What are the ambiguities for tokenization?

  \begin{solution}
    Ambiguities in tokenization include:
    \begin{itemize}
        \item **Hyphenation:** Is "state-of-the-art" one token or three?
        \item **Punctuation:** Should periods in abbreviations (e.g., "U.S.") be separated?
        \item **Contractions:** Should "don't" be split into "do" and "n't"?
        \item **Language differences:** Tokenization rules vary by language (e.g., Chinese characters vs. English words).
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{QA vs IR}
  What are the differences between QA and IR?

  \begin{solution}
    Differences between QA and IR:
    \begin{itemize}
        \item **Goal:**
        \begin{itemize}
            \item QA provides direct answers to user queries.
            \item IR retrieves documents that may contain relevant information.
        \end{itemize}
        \item **Processing:**
        \begin{itemize}
            \item QA requires natural language understanding and reasoning.
            \item IR focuses on keyword matching and ranking algorithms.
        \end{itemize}
        \item **Applications:**
        \begin{itemize}
            \item QA is used in chatbots and virtual assistants.
            \item IR powers search engines like Google.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Cross-language QA}
  What are the challenges in cross-language QA?

  \begin{solution}
    Challenges in cross-language QA include:
    \begin{itemize}
        \item **Translation errors:** Inaccurate translations can distort queries or answers.
        \item **Lack of parallel resources:** Few high-quality bilingual corpora exist for certain language pairs.
        \item **Cultural differences:** Questions or answers may need cultural adaptation.
        \item **Mismatched structures:** Linguistic differences make aligning queries and answers difficult.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Stemming vs lemmatization}
  What is the difference between stemming and lemmatization?

  \begin{solution}
    \begin{itemize}
        \item **Stemming:** Reduces a word to its root by chopping off affixes (e.g., "running" → "run"). It is less precise and may produce non-words.
        \item **Lemmatization:** Converts a word to its base form (lemma) based on its context (e.g., "better" → "good"). It is more accurate but computationally expensive.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Web corpus}
  Why are duplicates on the web, and how can they be detected?

  \begin{solution}
    \begin{itemize}
        \item **Reasons for duplicates:**
        \begin{itemize}
            \item Content mirroring across websites.
            \item Republishing of syndicated articles.
            \item Crawling the same site multiple times.
        \end{itemize}
        \item **Detection methods:**
        \begin{itemize}
            \item Hashing (e.g., MD5, SHA) to compare file contents.
            \item Cosine similarity or Jaccard index for text comparisons.
            \item Comparing HTML structure or metadata.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Web corpus}
  What are the 4 challenges when using the web as a corpus?

  \begin{solution}
    Challenges include:
    \begin{itemize}
        \item **Noisy data:** Web content often includes irrelevant or ungrammatical text.
        \item **Duplication:** Large-scale crawls often capture duplicate data.
        \item **Bias:** Web data may reflect societal or geographic biases.
        \item **Dynamic content:** Websites frequently change, making reproducibility difficult.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{TREC}
  What are the TREC question types?

  \begin{solution}
    TREC (Text REtrieval Conference) defines these question types:
    \begin{itemize}
        \item **Factoid questions:** Require specific factual answers (e.g., "Who is the president of the U.S.?").
        \item **List questions:** Require a list of items (e.g., "List countries in the EU").
        \item **Definition questions:** Seek explanations or definitions (e.g., "What is AI?").
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{QALD}
  What is QALD, and what are the challenges?

  \begin{solution}
    \begin{itemize}
        \item **Definition:** QALD (Question Answering over Linked Data) focuses on answering questions using linked data (e.g., RDF datasets).
        \item **Challenges:**
        \begin{itemize}
            \item Mapping natural language queries to structured data.
            \item Handling incomplete or ambiguous data in linked datasets.
            \item Scaling to large and heterogeneous data sources.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}



\setcounter{section}{2020}
\sheet{1st exam}

\begin{exercise}{Comprehensiveness}
  Explain comprehensiveness.

  \begin{solution}
    Comprehensiveness refers to the extent to which a system, dataset, or model covers all relevant aspects of a task or domain. In NLP:
    \begin{itemize}
        \item **Datasets:** Comprehensive datasets should include diverse and representative samples of the target domain.
        \item **Models:** Comprehensive models should handle all variations and exceptions within their scope.
        \item **Applications:** Comprehensive applications should meet all user requirements for a given task.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Complete the picture}
  Fill in the white boxes in the image.

  \begin{solution}
    To complete the image, analyze the given context and fill in the white boxes based on:
    \begin{itemize}
        \item **Labels:** Assign appropriate titles or labels to the boxes (e.g., steps in a pipeline).
        \item **Relationships:** Connect missing elements to ensure the flow aligns with the diagram's logic.
        \item **Annotations:** Use any provided hints to accurately reconstruct the intended content.
    \end{itemize}
    The exact answer depends on the image provided.
  \end{solution}
\end{exercise}

\begin{exercise}{NLTK}
  Explain two parameters in NLTK that start with a "c."

  \begin{solution}
    Two common NLTK parameters starting with "c" are:
    \begin{itemize}
        \item **Chunking:** Groups words into meaningful units (e.g., noun phrases) based on part-of-speech tagging.
        \item **Concordance:** Displays occurrences of a word within a context window, useful for lexical analysis.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{SVM}
  Explain Support Vector Machines when you have more than two classes.

  \begin{solution}
    For multi-class classification, SVM uses:
    \begin{itemize}
        \item **One-vs-One (OvO):** Trains a classifier for every pair of classes. Final classification is determined by majority voting.
        \item **One-vs-All (OvA):** Trains one classifier per class against all other classes. The highest-confidence prediction is chosen.
        \item **Extensions:** Kernel tricks and soft margins can be applied for better generalization.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Feature differences}
  Explain the differences in features between classical ML and deep network-based ML.

  \begin{solution}
    \begin{itemize}
        \item **Classical ML:**
        \begin{itemize}
            \item Relies on manually engineered features (e.g., TF-IDF, n-grams).
            \item Requires domain expertise for feature extraction.
        \end{itemize}
        \item **Deep learning:**
        \begin{itemize}
            \item Automatically learns hierarchical feature representations.
            \item Captures complex patterns (e.g., semantics, dependencies).
            \item Requires large datasets for training.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Cohen's Kappa}
  Calculate Cohen's Kappa for a given table.

  \begin{solution}
    Cohen's Kappa is calculated as:
    \[
    \kappa = \frac{P_o - P_e}{1 - P_e}
    \]
    Where:
    \begin{itemize}
        \item \(P_o\): Observed agreement (sum of diagonal elements divided by the total number of items).
        \item \(P_e\): Expected agreement (sum of product of marginal probabilities for each category).
    \end{itemize}
    Substitute the values from the given table into the formula to compute \(\kappa\).
  \end{solution}
\end{exercise}

\begin{exercise}{Evaluation metrics}
  Calculate Precision, Recall, and F1-Score.

  \begin{solution}
    Given true positives (\(TP\)), false positives (\(FP\)), and false negatives (\(FN\)):
    \begin{itemize}
        \item **Precision:** \( \frac{TP}{TP + FP} \)
        \item **Recall:** \( \frac{TP}{TP + FN} \)
        \item **F1-Score:** \( 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \)
    \end{itemize}
    Plug in the values provided to compute these metrics.
  \end{solution}
\end{exercise}

\begin{exercise}{IR and classification}
  Where is classification used in Information Retrieval?

  \begin{solution}
    Classification is used in IR for:
    \begin{itemize}
        \item **Query classification:** Identify user intent (e.g., navigational vs. informational).
        \item **Document classification:** Categorize documents into predefined topics or genres.
        \item **Relevance classification:** Predict whether a document is relevant to a query.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Translate datasets}
  List 3 reasons for and against translating an existing annotated dataset from German to another language.

  \begin{solution}
    \begin{itemize}
        \item **For:**
        \begin{itemize}
            \item Expands dataset availability to resource-scarce languages.
            \item Enables cross-lingual NLP research.
            \item Saves effort in recreating annotations.
        \end{itemize}
        \item **Against:**
        \begin{itemize}
            \item Potential loss of annotation quality in translation.
            \item Context or cultural relevance may not transfer.
            \item Requires manual review to ensure correctness.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Web corpus}
  List 3 reasons for and against using the web as a corpus.

  \begin{solution}
    \begin{itemize}
        \item **For:**
        \begin{itemize}
            \item Large-scale, diverse data sources.
            \item Includes up-to-date content.
            \item Covers multiple languages and domains.
        \end{itemize}
        \item **Against:**
        \begin{itemize}
            \item Noisy and unstructured data.
            \item Contains duplicates and redundant information.
            \item May include biased or low-quality content.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Pipeline}
  Given the pipeline from the first slide, assign three words to the individual steps.

  \begin{solution}
    Example:
    \begin{itemize}
        \item **Preprocessing:** Tokenization, normalization, and cleaning.
        \item **Feature extraction:** TF-IDF, embeddings.
        \item **Modeling:** Training, evaluation, and deployment.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{MRR and accuracy}
  Calculate MRR and accuracy for a Question/Answering system.

  \begin{solution}
    \begin{itemize}
        \item **Mean Reciprocal Rank (MRR):**
        \[
        MRR = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{\text{rank}_i}
        \]
        Where \(\text{rank}_i\) is the rank of the first correct answer for query \(i\).
        \item **Accuracy:** \(\frac{\text{Number of correct answers}}{\text{Total number of queries}}\)
    \end{itemize}
    Calculate based on the provided data.
  \end{solution}
\end{exercise}

\begin{exercise}{Language detection}
  Explain language detection.

  \begin{solution}
    Language detection involves identifying the language of a given text. Common approaches:
    \begin{itemize}
        \item **N-gram analysis:** Compare character n-grams to language profiles.
        \item **Lexicon-based methods:** Match text to language-specific dictionaries.
        \item **Machine learning:** Train classifiers (e.g., SVM, neural networks) on labeled text.
        \item **Hybrid models:** Combine multiple techniques for higher accuracy.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Plagiarism detection}
  Explain how to find plagiarism.

  \begin{solution}
    Plagiarism detection methods:
    \begin{itemize}
        \item **String matching:** Use algorithms to find exact matches between documents.
        \item **Semantic similarity:** Apply embeddings to detect paraphrased or rephrased content.
        \item **Citation analysis:** Check for missing or incorrect citations.
        \item **Fingerprinting:** Generate hashes of text fragments and compare across sources.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Crowdsourcing}
  List the pros and cons of crowdsourcing.

  \begin{solution}
    \begin{itemize}
        \item **Pros:**
        \begin{itemize}
            \item Fast and scalable data annotation.
            \item Access to diverse annotators.
            \item Cost-effective compared to hiring experts.
        \end{itemize}
        \item **Cons:**
        \begin{itemize}
            \item Quality control is challenging.
            \item May introduce noise or bias.
            \item Difficult to ensure annotator expertise.
        \end{itemize}
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Recall improvement}
  Explain how to improve recall when using a search engine without changing the algorithm.

  \begin{solution}
    Recall can be improved by:
    \begin{itemize}
        \item **Expanding queries:** Use synonyms, related terms, or stemming to match more results.
        \item **Reducing filters:** Remove restrictive criteria (e.g., date, location).
        \item **Boosting document recall:** Index more documents or reduce relevance thresholds.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Spell checker}
  Explain how a spell checker works with non-words.

  \begin{solution}
    For non-words, spell checkers:
    \begin{itemize}
        \item **Dictionary lookup:** Identify words not in the dictionary.
        \item **Edit distance:** Suggest corrections based on similarity (e.g., Levenshtein distance).
        \item **Contextual models:** Use surrounding words to predict the intended word.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{IR cycle}
  Explain the Information Retrieval cycle.

  \begin{solution}
    The IR cycle includes:
    \begin{itemize}
        \item **Indexing:** Process and store data for efficient retrieval.
        \item **Querying:** Users submit queries to search the index.
        \item **Retrieval:** Return ranked results based on relevance scores.
        \item **Evaluation:** Assess performance using metrics like precision and recall.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Vector space}
  Explain Vector Space in Information Retrieval.

  \begin{solution}
    The Vector Space Model represents documents and queries as vectors:
    \begin{itemize}
        \item **Terms:** Each dimension corresponds to a term in the vocabulary.
        \item **Weights:** TF-IDF scores are used as weights for each term.
        \item **Similarity:** Compute cosine similarity to rank documents by relevance to the query.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sentiment analysis}
  Explain the difference between factual sentiment analysis and opinionated analysis.

  \begin{solution}
    \begin{itemize}
        \item **Factual sentiment analysis:** Focuses on extracting objective sentiment (e.g., product ratings).
        \item **Opinionated analysis:** Identifies subjective opinions (e.g., customer reviews).
        \item **Key difference:** Factual analysis is more structured and easier to evaluate, whereas opinionated analysis is subjective and context-dependent.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Sentiment analysis}
  Explain the areas where sentiment analysis is used.

  \begin{solution}
    Sentiment analysis applications:
    \begin{itemize}
        \item **Customer feedback:** Analyze reviews for product improvements.
        \item **Social media monitoring:** Detect trends and public sentiment.
        \item **Political analysis:** Gauge public opinion on policies or events.
        \item **Market research:** Understand consumer preferences.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Teacher/Student}
  Explain Teacher/Student and why it is better than just audio.

  \begin{solution}
    The Teacher/Student approach combines:
    \begin{itemize}
        \item **Teacher signals:** Provide detailed instructions and explanations.
        \item **Student feedback:** Allows iterative learning and personalization.
        \item **Better than audio:** Interaction and adaptivity enhance understanding and retention compared to static audio content.
    \end{itemize}
  \end{solution}
\end{exercise}

\begin{exercise}{Representation and fusion}
  Explain representation and fusion and the differences.

  \begin{solution}
    \begin{itemize}
        \item **Representation:** Encodes data (e.g., text or images) into feature vectors.
        \item **Fusion:** Combines multiple data modalities (e.g., text and images) to improve performance.
        \item **Differences:** Representation focuses on individual modalities, whereas fusion integrates multiple modalities into a unified representation.
    \end{itemize}
  \end{solution}
\end{exercise}

\end{document}